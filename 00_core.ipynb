{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerias de rapid\n",
    "import math\n",
    "from ebm.models import RBM\n",
    "from ebm.optimizers import SGD, Adam, outer_product\n",
    "from ebm.samplers import ContrastiveDivergence as cd\n",
    "from torch import cat, einsum, max, mm, randint, rand_like, sigmoid, sign,     \\\n",
    "                  sqrt, tanh, zeros_like\n",
    "from torch.nn.functional import linear, dropout\n",
    "from torch.nn import Parameter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def say_hello(to):\n",
    "    \"Say hello to somebody\"\n",
    "    return f'Hello {to}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Eloy!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say_hello(\"Eloy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert say_hello(\"Jeremy\")==\"Hello Jeremy!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Models\n",
    "# -----------------------------------------------------------------------------\n",
    "class RBM_pm(RBM):\n",
    "\n",
    "    def __init__(self, n_visible=10, n_hidden=50, sampler=None, optimizer=None,\n",
    "                 device=None, weights=None):\n",
    "        '''Restricted Boltzmann machine with spin-like neurons (their allowed\n",
    "           values are +1/-1) instead of binary (0/1) neurons, and with no\n",
    "           biases.\n",
    "           \n",
    "        Arguments:\n",
    "           \n",
    "            :param n_visible: The number nodes in the visible layer\n",
    "            :type n_visible: int\n",
    "            :param n_hidden: The number nodes in the hidden layer\n",
    "            :type n_hidden: int\n",
    "            :param sampler: Method used to draw samples from the model\n",
    "            :type sampler: :class:`ebm.samplers`\n",
    "            :param optimizer: Optimizer used for parameter updates\n",
    "            :type optimizer: :class:`ebm.optimizers`\n",
    "            :param device: Device where to perform computations. None is CPU.\n",
    "            :type device: torch.device\n",
    "            :param W: Optional parameter to specify the weights of the RBM\n",
    "            :type W: torch.nn.Parameter\n",
    "            :param vbias: Optional parameter to specify the visible biases of\n",
    "                          the RBM\n",
    "            :type vbias: torch.nn.Parameter\n",
    "            :param hbias: Optional parameter to specify the hidden biases of\n",
    "                          the RBM\n",
    "            :type hbias: torch.nn.Parameter\n",
    "        '''\n",
    "        super().__init__(n_visible, n_hidden, sampler, optimizer, device, weights)\n",
    "    \n",
    "    def free_energy(self, v):\n",
    "        '''Computes the free energy for a given state of the visible layer.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "            :param v: The state of the visible layer of the RBM\n",
    "            :type v: torch.Tensor\n",
    "\n",
    "            :returns: torch.Tensor\n",
    "        '''\n",
    "        \n",
    "        wx_b = linear(v, self.weights, self.hbias)\n",
    "        \n",
    "        # Fancy (and overflow-resistant) way of computing log(2cosh(x))\n",
    "        a = max(wx_b, -wx_b)\n",
    "        hidden_term = (a + ((-wx_b - a).exp() + (wx_b - a).exp()).log()).sum(1)\n",
    "        return -hidden_term\n",
    "\n",
    "    def train(self, input_data):\n",
    "        '''Trains the RBM.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "            :param input_data: Batch of training points\n",
    "            :type input_data: torch.utils.data.DataLoader\n",
    "        '''\n",
    "        for batch in tqdm(input_data, desc=('Epoch ' +\n",
    "                                            str(self.optimizer.epoch + 1))):\n",
    "            \n",
    "            sample_data = batch.float()\n",
    "            \n",
    "            vpos = sample_data\n",
    "            vneg = self.sampler.get_negative_sample(vpos, self.weights,\n",
    "                                                   self.vbias, self.hbias)\n",
    "            W_update, _, _ = \\\n",
    "                             self.optimizer.get_updates(vpos, vneg,\n",
    "                                                self.weights, self.vbias, self.hbias)\n",
    "            self.weights += W_update\n",
    "             \n",
    "        self.optimizer.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RA_RBM(RBM_pm):\n",
    "\n",
    "    def __init__(self, n_visible=100, n_hidden=50, K=50,\n",
    "                 optimizer=None, device=None, xi=None):\n",
    "        '''RBM where the weights are computed through the method of Restricted\n",
    "           Axon. The weights are computed from low-energy patterns, and the\n",
    "           parameters to be optimized is the patterns themselves\n",
    "        \n",
    "        Arguments:\n",
    "            \n",
    "            :param n_visible: The number nodes in the visible layer\n",
    "            :type n_visible: int\n",
    "            :param n_hidden: The number nodes in the hidden layer\n",
    "            :type n_hidden: int\n",
    "            :param K: The number of patterns from which the weights are computed\n",
    "            :type K: int\n",
    "            :param sampler: Method used to draw samples from the model\n",
    "            :type sampler: :class:`samplers`\n",
    "            :param optimizer: Optimizer used for parameter updates\n",
    "            :type optimizer: :class:`optimizers`\n",
    "            :param device: Device where to perform computations. None is CPU.\n",
    "            :type device: torch.device\n",
    "            :param xi: Optional parameter to specify the initial patterns\n",
    "            :type xi: torch.nn.Parameter\n",
    "            :param vbias: Optional parameter to specify the visible biases of\n",
    "                          the RBM\n",
    "            :type vbias: torch.nn.Parameter\n",
    "            :param hbias: Optional parameter to specify the hidden biases of\n",
    "                          the RBM\n",
    "            :type hbias: torch.nn.Parameter'''\n",
    "        super().__init__(n_visible, n_hidden, 'None', optimizer, device)\n",
    "        self.K         = K\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden  = n_hidden\n",
    "        \n",
    "        if xi is not None:\n",
    "            self.xi = xi\n",
    "        else:\n",
    "            self.xi = Parameter((2 * randint(0,\n",
    "                                             2,\n",
    "                                        (self.K, self.n_hidden + self.n_visible)\n",
    "                                             ) - 1).float().to(self.device))\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False  \n",
    "\n",
    "        self.get_params()\n",
    "            \n",
    "    def get_params(self):\n",
    "        '''Computes the weight matrix of the RBM from the patterns'''\n",
    "        vis  = self.xi[:, :self.n_visible]\n",
    "        hidd = self.xi[:, self.n_visible:]\n",
    "        self.weights.data = (outer_product(hidd, vis).sum(0)).to(self.device)\n",
    "        self.weights.data /= math.sqrt(self.K)\n",
    "        \n",
    "    def train(self, input_data):\n",
    "        '''Trains the RBM.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "            :param input_data: Batch of training points\n",
    "            :type input_data: torch.utils.data.DataLoader\n",
    "        '''\n",
    "        \n",
    "        for batch in tqdm(input_data, desc=('Epoch ' +\n",
    "                                            str(self.optimizer.epoch + 1))):\n",
    "\n",
    "            sample_data = batch.float()\n",
    "\n",
    "            vpos = sample_data\n",
    "            # Get negative phase from the patterns\n",
    "            vneg = sign(self.xi[:, :vpos.shape[1]])\n",
    "\n",
    "            xi_update = self.optimizer.get_updates(vpos, vneg, self.xi)\n",
    "            self.xi += xi_update\n",
    "\n",
    "            self.get_params()\n",
    "        \n",
    "        # Renormalize after the training epoch has concluded\n",
    "        self.xi.data = sign(self.xi)\n",
    "        self.get_params()\n",
    "        self.optimizer.epoch += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Optimizers\n",
    "# -----------------------------------------------------------------------------\n",
    "class SGD_xi(SGD):\n",
    "    \n",
    "    def __init__(self, learning_rate, momentum=0, weight_decay=0):\n",
    "        '''Update the value of the pattern units via Stochastic Gradient Descent\n",
    "        \n",
    "        Arguments:\n",
    "\n",
    "            :param learning_rate: Learning rate\n",
    "            :type learning_rate: float\n",
    "            :param weight_decay: Weight decay parameter, to prevent overfitting\n",
    "            :type weight_decay: float\n",
    "            :param momentum: Momentum parameter, for improved learning\n",
    "            :type momentum: float\n",
    "        '''\n",
    "        super().__init__(learning_rate, momentum, weight_decay)\n",
    "\n",
    "    def get_params(self, xi, n_visible):\n",
    "        '''Computes the neuron connections (axons) of the RBM from the patterns\n",
    "        using the Hebbian rule\n",
    "        \n",
    "        Arguments:\n",
    "\n",
    "            :param xi: Patterns from which to compute the weights\n",
    "            :type xi: torch.Tensor\n",
    "            :param n_visible: Number of visible neurons in the model\n",
    "            :type n_visible: int\n",
    "        '''\n",
    "        vis = xi[:, :n_visible]\n",
    "        hidd = xi[:, n_visible:]\n",
    "        W = (mm(hidd.t(), vis) / math.sqrt(xi.shape[0])).to(xi.device)\n",
    "        return W\n",
    "\n",
    "    def get_updates(self, vpos, vneg, xi):\n",
    "        '''Obtain the parameter updates\n",
    "        \n",
    "        Arguments:\n",
    "\n",
    "            :param vpos: Batch of samples from the training set\n",
    "            :type vpos: torch.Tensor\n",
    "            :param vneg: Batch of samples drawn from the model\n",
    "            :type vneg: torch.Tensor\n",
    "            :param xi: Patterns from which to compute the weights\n",
    "            :type xi: torch.Tensor\n",
    "        '''\n",
    "        if self.first_call:\n",
    "            self.n_vis      = vpos.shape[1]\n",
    "            self.pos_batch  = vpos.shape[0]\n",
    "            self.neg_batch  = vneg.shape[0]\n",
    "            self.K          = xi.shape[0]\n",
    "            self.xi_update  = zeros_like(xi)\n",
    "            self.first_call = False\n",
    "\n",
    "        self.xi_update *= self.momentum\n",
    "        self.xi_update -= self.learning_rate * self.weight_decay * xi\n",
    "        \n",
    "        W = self.get_params(xi, self.n_vis)\n",
    "        xi_vis = xi[:, :self.n_vis]\n",
    "        xi_hid = xi[:, self.n_vis:]\n",
    "        deltaxi_v = (einsum('bj,bk->kj', (vpos, mm(tanh(mm(vpos, W.t())), xi_hid.t()))) / self.pos_batch\n",
    "                   - einsum('bj,bk->kj', (vneg, mm(tanh(mm(vneg, W.t())), xi_hid.t()))) / self.neg_batch) / math.sqrt(self.K)\n",
    "        deltaxi_h = (einsum('bj,kj,ba->ka',(vpos, xi_vis, tanh(mm(vpos, W.t())))) / self.pos_batch\n",
    "                   - einsum('bj,kj,ba->ka',(vneg, xi_vis, tanh(mm(vneg, W.t())))) / self.neg_batch) / math.sqrt(self.K)\n",
    "        deltaxi = cat([deltaxi_v, deltaxi_h], 1)\n",
    "        self.xi_update.data += self.learning_rate * deltaxi\n",
    "\n",
    "        return self.xi_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
