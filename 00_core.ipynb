{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#librerias de rapid\n",
    "import math\n",
    "from ebm.models import RBM\n",
    "from ebm.optimizers import SGD, Adam, outer_product\n",
    "from ebm.samplers import ContrastiveDivergence as cd\n",
    "from torch import cat, einsum, max, mm, randint, rand_like, sigmoid, sign,     \\\n",
    "                  sqrt, tanh, zeros_like\n",
    "from torch.nn.functional import linear, dropout\n",
    "from torch.nn import Parameter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#Librerias de utils\n",
    "import gc\n",
    "import torch\n",
    "from itertools import combinations\n",
    "from torch import cat, max, min, ones, randn, Tensor, zeros\n",
    "from torch.nn.functional import linear\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def say_hello(to):\n",
    "    \"Say hello to somebody\"\n",
    "    return f'Hello {to}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Eloy!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say_hello(\"Eloy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert say_hello(\"Jeremy\")==\"Hello Jeremy!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Las clases relevantes de rapid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# -----------------------------------------------------------------------------\n",
    "# Models\n",
    "# -----------------------------------------------------------------------------\n",
    "class RBM_pm(RBM):\n",
    "\n",
    "    def __init__(self, n_visible=10, n_hidden=50, sampler=None, optimizer=None,\n",
    "                 device=None, weights=None):\n",
    "        '''Restricted Boltzmann machine with spin-like neurons (their allowed\n",
    "           values are +1/-1) instead of binary (0/1) neurons, and with no\n",
    "           biases.\n",
    "           \n",
    "        Arguments:\n",
    "           \n",
    "            :param n_visible: The number nodes in the visible layer\n",
    "            :type n_visible: int\n",
    "            :param n_hidden: The number nodes in the hidden layer\n",
    "            :type n_hidden: int\n",
    "            :param sampler: Method used to draw samples from the model\n",
    "            :type sampler: :class:`ebm.samplers`\n",
    "            :param optimizer: Optimizer used for parameter updates\n",
    "            :type optimizer: :class:`ebm.optimizers`\n",
    "            :param device: Device where to perform computations. None is CPU.\n",
    "            :type device: torch.device\n",
    "            :param W: Optional parameter to specify the weights of the RBM\n",
    "            :type W: torch.nn.Parameter\n",
    "            :param vbias: Optional parameter to specify the visible biases of\n",
    "                          the RBM\n",
    "            :type vbias: torch.nn.Parameter\n",
    "            :param hbias: Optional parameter to specify the hidden biases of\n",
    "                          the RBM\n",
    "            :type hbias: torch.nn.Parameter\n",
    "        '''\n",
    "        super().__init__(n_visible, n_hidden, sampler, optimizer, device, weights)\n",
    "    \n",
    "    def free_energy(self, v):\n",
    "        '''Computes the free energy for a given state of the visible layer.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "            :param v: The state of the visible layer of the RBM\n",
    "            :type v: torch.Tensor\n",
    "\n",
    "            :returns: torch.Tensor\n",
    "        '''\n",
    "        \n",
    "        wx_b = linear(v, self.weights, self.hbias)\n",
    "        \n",
    "        # Fancy (and overflow-resistant) way of computing log(2cosh(x))\n",
    "        a = max(wx_b, -wx_b)\n",
    "        hidden_term = (a + ((-wx_b - a).exp() + (wx_b - a).exp()).log()).sum(1)\n",
    "        return -hidden_term\n",
    "\n",
    "    def train(self, input_data):\n",
    "        '''Trains the RBM.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "            :param input_data: Batch of training points\n",
    "            :type input_data: torch.utils.data.DataLoader\n",
    "        '''\n",
    "        for batch in tqdm(input_data, desc=('Epoch ' +\n",
    "                                            str(self.optimizer.epoch + 1))):\n",
    "            \n",
    "            sample_data = batch.float()\n",
    "            \n",
    "            vpos = sample_data\n",
    "            vneg = self.sampler.get_negative_sample(vpos, self.weights,\n",
    "                                                   self.vbias, self.hbias)\n",
    "            W_update, _, _ = \\\n",
    "                             self.optimizer.get_updates(vpos, vneg,\n",
    "                                                self.weights, self.vbias, self.hbias)\n",
    "            self.weights += W_update\n",
    "             \n",
    "        self.optimizer.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RA_RBM(RBM_pm):\n",
    "\n",
    "    def __init__(self, n_visible=100, n_hidden=50, K=50,\n",
    "                 optimizer=None, device=None, xi=None):\n",
    "        '''RBM where the weights are computed through the method of Restricted\n",
    "           Axon. The weights are computed from low-energy patterns, and the\n",
    "           parameters to be optimized is the patterns themselves\n",
    "        \n",
    "        Arguments:\n",
    "            \n",
    "            :param n_visible: The number nodes in the visible layer\n",
    "            :type n_visible: int\n",
    "            :param n_hidden: The number nodes in the hidden layer\n",
    "            :type n_hidden: int\n",
    "            :param K: The number of patterns from which the weights are computed\n",
    "            :type K: int\n",
    "            :param sampler: Method used to draw samples from the model\n",
    "            :type sampler: :class:`samplers`\n",
    "            :param optimizer: Optimizer used for parameter updates\n",
    "            :type optimizer: :class:`optimizers`\n",
    "            :param device: Device where to perform computations. None is CPU.\n",
    "            :type device: torch.device\n",
    "            :param xi: Optional parameter to specify the initial patterns\n",
    "            :type xi: torch.nn.Parameter\n",
    "            :param vbias: Optional parameter to specify the visible biases of\n",
    "                          the RBM\n",
    "            :type vbias: torch.nn.Parameter\n",
    "            :param hbias: Optional parameter to specify the hidden biases of\n",
    "                          the RBM\n",
    "            :type hbias: torch.nn.Parameter'''\n",
    "        super().__init__(n_visible, n_hidden, 'None', optimizer, device)\n",
    "        self.K         = K\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden  = n_hidden\n",
    "        \n",
    "        if xi is not None:\n",
    "            self.xi = xi\n",
    "        else:\n",
    "            self.xi = Parameter((2 * randint(0,\n",
    "                                             2,\n",
    "                                        (self.K, self.n_hidden + self.n_visible)\n",
    "                                             ) - 1).float().to(self.device))\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False  \n",
    "\n",
    "        self.get_params()\n",
    "            \n",
    "    def get_params(self):\n",
    "        '''Computes the weight matrix of the RBM from the patterns'''\n",
    "        vis  = self.xi[:, :self.n_visible]\n",
    "        hidd = self.xi[:, self.n_visible:]\n",
    "        self.weights.data = (outer_product(hidd, vis).sum(0)).to(self.device)\n",
    "        self.weights.data /= math.sqrt(self.K)\n",
    "        \n",
    "    def train(self, input_data):\n",
    "        '''Trains the RBM.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "            :param input_data: Batch of training points\n",
    "            :type input_data: torch.utils.data.DataLoader\n",
    "        '''\n",
    "        \n",
    "        for batch in tqdm(input_data, desc=('Epoch ' +\n",
    "                                            str(self.optimizer.epoch + 1))):\n",
    "\n",
    "            sample_data = batch.float()\n",
    "\n",
    "            vpos = sample_data\n",
    "            # Get negative phase from the patterns\n",
    "            vneg = sign(self.xi[:, :vpos.shape[1]])\n",
    "\n",
    "            xi_update = self.optimizer.get_updates(vpos, vneg, self.xi)\n",
    "            self.xi += xi_update\n",
    "\n",
    "            self.get_params()\n",
    "        \n",
    "        # Renormalize after the training epoch has concluded\n",
    "        self.xi.data = sign(self.xi)\n",
    "        self.get_params()\n",
    "        self.optimizer.epoch += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# -----------------------------------------------------------------------------\n",
    "# Optimizers\n",
    "# -----------------------------------------------------------------------------\n",
    "class SGD_xi(SGD):\n",
    "    \n",
    "    def __init__(self, learning_rate, momentum=0, weight_decay=0):\n",
    "        '''Update the value of the pattern units via Stochastic Gradient Descent\n",
    "        \n",
    "        Arguments:\n",
    "\n",
    "            :param learning_rate: Learning rate\n",
    "            :type learning_rate: float\n",
    "            :param weight_decay: Weight decay parameter, to prevent overfitting\n",
    "            :type weight_decay: float\n",
    "            :param momentum: Momentum parameter, for improved learning\n",
    "            :type momentum: float\n",
    "        '''\n",
    "        super().__init__(learning_rate, momentum, weight_decay)\n",
    "\n",
    "    def get_params(self, xi, n_visible):\n",
    "        '''Computes the neuron connections (axons) of the RBM from the patterns\n",
    "        using the Hebbian rule\n",
    "        \n",
    "        Arguments:\n",
    "\n",
    "            :param xi: Patterns from which to compute the weights\n",
    "            :type xi: torch.Tensor\n",
    "            :param n_visible: Number of visible neurons in the model\n",
    "            :type n_visible: int\n",
    "        '''\n",
    "        vis = xi[:, :n_visible]\n",
    "        hidd = xi[:, n_visible:]\n",
    "        W = (mm(hidd.t(), vis) / math.sqrt(xi.shape[0])).to(xi.device)\n",
    "        return W\n",
    "\n",
    "    def get_updates(self, vpos, vneg, xi):\n",
    "        '''Obtain the parameter updates\n",
    "        \n",
    "        Arguments:\n",
    "\n",
    "            :param vpos: Batch of samples from the training set\n",
    "            :type vpos: torch.Tensor\n",
    "            :param vneg: Batch of samples drawn from the model\n",
    "            :type vneg: torch.Tensor\n",
    "            :param xi: Patterns from which to compute the weights\n",
    "            :type xi: torch.Tensor\n",
    "        '''\n",
    "        if self.first_call:\n",
    "            self.n_vis      = vpos.shape[1]\n",
    "            self.pos_batch  = vpos.shape[0]\n",
    "            self.neg_batch  = vneg.shape[0]\n",
    "            self.K          = xi.shape[0]\n",
    "            self.xi_update  = zeros_like(xi)\n",
    "            self.first_call = False\n",
    "\n",
    "        self.xi_update *= self.momentum\n",
    "        self.xi_update -= self.learning_rate * self.weight_decay * xi\n",
    "        \n",
    "        W = self.get_params(xi, self.n_vis)\n",
    "        xi_vis = xi[:, :self.n_vis]\n",
    "        xi_hid = xi[:, self.n_vis:]\n",
    "        deltaxi_v = (einsum('bj,bk->kj', (vpos, mm(tanh(mm(vpos, W.t())), xi_hid.t()))) / self.pos_batch\n",
    "                   - einsum('bj,bk->kj', (vneg, mm(tanh(mm(vneg, W.t())), xi_hid.t()))) / self.neg_batch) / math.sqrt(self.K)\n",
    "        deltaxi_h = (einsum('bj,kj,ba->ka',(vpos, xi_vis, tanh(mm(vpos, W.t())))) / self.pos_batch\n",
    "                   - einsum('bj,kj,ba->ka',(vneg, xi_vis, tanh(mm(vneg, W.t())))) / self.neg_batch) / math.sqrt(self.K)\n",
    "        deltaxi = cat([deltaxi_v, deltaxi_h], 1)\n",
    "        self.xi_update.data += self.learning_rate * deltaxi\n",
    "\n",
    "        return self.xi_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aqui empiezan las funciones y la clase de utils para el ejemplo de training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GradientRBM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_visible=10, n_hidden=50, device=None, weights=None):\n",
    "        '''Restricted Boltzmann machine with spin-like neurons to be trained via\n",
    "        AutoML in PyTorch.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "            :param n_visible: The number nodes in the visible layer\n",
    "            :type n_visible: int\n",
    "            :param n_hidden: The number nodes in the hidden layer\n",
    "            :type n_hidden: int\n",
    "            :param device: Device where to perform computations. None is CPU.\n",
    "            :type device: torch.device\n",
    "            :param W: Optional parameter to specify the weights of the RBM\n",
    "            :type W: torch.nn.Parameter\n",
    "        '''\n",
    "        super(GradientRBM, self).__init__()\n",
    "\n",
    "        if device is not None:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        if weights is not None:\n",
    "            self.weights = torch.nn.Parameter(weights.to(self.device),\n",
    "                                              requires_grad=True)\n",
    "        else:\n",
    "            self.weights = torch.nn.Parameter(Tensor(\n",
    "                                               0.01 * randn(n_hidden, n_visible)\n",
    "                                                     ).to(self.device),\n",
    "                                              requires_grad=True)\n",
    "    def free_energy(self, v):\n",
    "        wx_b = linear(v, self.weights)\n",
    "        a    = max(wx_b, -wx_b)\n",
    "\n",
    "        hidden_term = (a + ((-wx_b - a).exp() + (wx_b - a).exp()).log()).sum(1)\n",
    "        return -hidden_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_bars_4x4():\n",
    "    '''Create the 4x4-pixel Bars datasets, with flattened images\n",
    "\n",
    "        :returns train_data: torch.Tensor of size (10,16) with training images\n",
    "        :returns recon_train_data: torch.Tensor of size (10,16) with\n",
    "                                   partially-erased training images\n",
    "        :returns test_data: torch.Tensor of size (4,16) with testing images\n",
    "        :returns recon_test_data: torch.Tensor of size (4,16) with\n",
    "                                  partially-erased testing images\n",
    "    '''\n",
    "    train_1    = Tensor([[1, -1, -1, -1]] * 4).unsqueeze(0).view((-1, 16))\n",
    "    train_2    = Tensor([[1, 1, -1, -1]] * 4).unsqueeze(0).view((-1, 16))\n",
    "    train_3    = Tensor([[1, 1, 1, -1]] * 4).unsqueeze(0).view((-1, 16))\n",
    "    train_4    = Tensor([[1, -1, 1, 1]] * 4).unsqueeze(0).view((-1, 16))\n",
    "    train_5    = Tensor([[1, 1, -1, 1]] * 4).unsqueeze(0).view((-1, 16))\n",
    "    train      = [train_1, train_2, train_3, train_4, train_5]\n",
    "    inv_train  = [-image for image in train]\n",
    "    train_data = cat(train + inv_train, 0)\n",
    "\n",
    "    recon_train_data = train_data.clone()\n",
    "    recon_train_data[:,4:] = 0\n",
    "\n",
    "    test_1    = Tensor([[1, -1, 1, -1]] * 4).unsqueeze(0).view((-1, 16))\n",
    "    test_2    = Tensor([[1, -1, -1, 1]] * 4).unsqueeze(0).view((-1, 16))\n",
    "    test      = [test_1, test_2]\n",
    "    inv_test  = [-image for image in test]\n",
    "    test_data = cat(test + inv_test, 0)\n",
    "\n",
    "    recon_test_data = test_data.clone()\n",
    "    recon_test_data[:,4:] = 0\n",
    "\n",
    "    return [train_data, recon_train_data, test_data, recon_test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def logsumexp(tensor):\n",
    "    '''Computes pointwise log(sum(exp())) for all elements in a torch tensor.\n",
    "    The way of computing it without under- or overflows is through the\n",
    "    log-sum-exp trick, namely computing\n",
    "    log(1+exp(x)) = a + log(exp(-a) + exp(x-a))     with a = max(0, x)\n",
    "    The function is adapted to be used in GPU if needed.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        :param tensor: torch.Tensor\n",
    "        :returns: torch.Tensor\n",
    "    '''\n",
    "    a = max(zeros(1).to(tensor.device), max(tensor))\n",
    "    return a + (tensor - a).exp().sum().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
